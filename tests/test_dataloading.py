from numpy.core.numeric import cross
import seaborn as sbn
import ray
from ray import tune
from ray.tune import CLIReporter
from ray.tune import Analysis
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Type, Union
from typing import cast, no_type_check
from typing_extensions import Literal

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pytest
import seaborn as sbn
from numpy import ndarray
from pandas import DataFrame, Series
import pytest
from tests.loading import load_diabetes, load_park, load_trans, load_SPECT
from sklearn.ensemble import RandomForestClassifier as RF
from sklearn.ensemble import AdaBoostClassifier as AdaBoost
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression as LR
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import cross_val_score
from typing import Any

Dataset = Literal["Diabetes", "Transfusion", "Parkinsons", "SPECT"]


def print_results(analysis: Analysis, cols: List[str], dataset: Dataset) -> DataFrame:
    df = analysis.dataframe("acc")
    df = df.loc[:, cols]
    df.sort_values(by="acc", ascending=False, inplace=True)
    renamed_cols = [col.replace("config/", "") for col in df.columns]
    df.columns = renamed_cols
    print("\n")
    print(df.to_markdown(tablefmt="pretty"))
    print(f"Best config for {dataset}: ", analysis.get_best_config(metric="acc", mode="max"))
    return df


def objective_function(model: Any, model_args: Dict = dict()) -> Callable:
    def objective(x: ndarray, y: ndarray, config: Dict) -> float:
        m = model(**model_args, **config)
        return float(np.mean(cross_val_score(m, x, y, cv=5)))
        # x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2)
        # m = model(**model_args, **config)
        # m.fit(x_train, y_train)
        # return float(m.score(x_test, y_test))

    return objective


def train_function(objective: Callable, dataset: str) -> Callable:
    mapping = {
        "Diabetes": load_diabetes,
        "Parkinsons": load_park,
        "Transfusion": load_trans,
        "SPECT": load_SPECT,
    }
    x, y = mapping[dataset]()
    # x = StandardScaler().fit_transform(x, y)
    x = MinMaxScaler().fit_transform(x, y)

    def train(config: Dict) -> None:
        acc = objective(x, y, config)
        tune.report(acc=acc)

    return train


@pytest.mark.fast
def test_loading_functions() -> None:
    x, y = load_diabetes()
    assert x.shape == (768, 8)
    assert y.shape == (768,)
    assert len(np.unique(y)) == 2

    x, y = load_park()
    assert x.shape == (195, 22)
    assert y.shape == (195,)
    assert len(np.unique(y)) == 2

    x, y = load_trans()
    assert x.shape == (748, 4)
    assert y.shape == (748,)
    assert len(np.unique(y)) == 2

    x, y = load_SPECT()
    assert x.shape == (267, 22)
    assert y.shape == (267,)
    assert len(np.unique(y)) == 2


def test_mlp_params(capsys: Any) -> None:
    ray.init(num_cpus=8)
    for DATASET in ["Diabetes", "Transfusion", "Parkinsons", "SPECT"]:
        # for DATASET in ["Parkinsons", "SPECT"]:

        def objective(
            x: ndarray,
            y: ndarray,
            alpha: float,
            layer: int,
            # layer1: int,
            # layer2: int,
            # layer3: int,
            # layer4: int,
            # layer5: int,
            # layer6: int,
            iter: int,
        ) -> float:
            mlp = MLPClassifier(
                # (layer1, layer2, layer3, layer4, layer5, layer6),
                (layer, layer, layer, layer, layer, layer),
                batch_size=32,
                alpha=alpha,
                max_iter=iter,
            )
            return np.mean(cross_val_score(mlp, x, y, cv=5))
            x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2)
            mlp = MLPClassifier(
                # (layer1, layer2, layer3, layer4, layer5, layer6),
                (layer, layer, layer, layer, layer, layer),
                batch_size=32,
                alpha=alpha,
                max_iter=iter,
            )
            mlp.fit(x_train, y_train)
            return float(mlp.score(x_test, y_test))

        def train(config: Any) -> None:
            x, y = load_diabetes()
            # x, y = load_trans()
            # x, y = load_park()
            # x, y = load_SPECT()
            x = StandardScaler().fit_transform(x, y)
            alpha = config["alpha"]
            layer = config["layer"]
            # layer1 = config["layer1"]
            # layer2 = config["layer2"]
            # layer3 = config["layer3"]
            # layer4 = config["layer4"]
            # layer5 = config["layer5"]
            # layer6 = config["layer6"]
            iters = config["iter"]
            # acc = objective(x, y, alpha, layer1, layer2, layer3, layer4, layer5, layer6, iters)
            acc = objective(x, y, alpha, layer, iters)
            tune.report(acc=acc)

        config = {
            "alpha": tune.qloguniform(1e-6, 1e-2, 5e-7),
            # "layer": tune.choice([4, 8, 16, 32, 64]),
            # "layer": tune.choice([4, 8, 16]),
            "layer": tune.choice([4, 8, 16, 32]),
            # "layer1": tune.choice([8, 16, 32, 64]),
            # "layer2": tune.choice([8, 16, 32, 64]),
            # "layer3": tune.choice([8, 16, 32, 64]),
            # "layer4": tune.choice([8, 16, 32, 64]),
            # "layer5": tune.choice([8, 16, 32, 64]),
            # "layer6": tune.choice([8, 16, 32, 64]),
            "iter": tune.choice([750, 1000]),
        }
        cols = ["acc", *list(map(lambda k: f"config/{k}", config.keys()))]

        # analysis = tune.run(train, config=config, num_samples=128)
        analysis = tune.run(train, config=config, num_samples=64)
        with capsys.disabled():
            print_results(analysis, cols, DATASET)


def test_rf_params(capsys: Any) -> None:
    ray.init(num_cpus=8)
    for DATASET in ["Diabetes", "Transfusion", "Parkinsons", "SPECT"]:

        objective = objective_function(RF)
        train = train_function(objective, DATASET)
        config = {
            "n_estimators": tune.choice([10, 20, 50, 100, 200, 400]),
            "min_samples_leaf": tune.randint(1, 5),
            "max_features": tune.choice(["auto", "log2", None, 0.1, 0.25, 0.5, 0.75]),
            "max_depth": tune.choice([None, 2, 4, 6, 8, 10, 20]),
        }
        cols = ["acc", *list(map(lambda k: f"config/{k}", config.keys()))]
        analysis = tune.run(train, config=config, num_samples=250)
        with capsys.disabled():
            print_results(analysis, cols, DATASET)


def test_ada_params(capsys: Any) -> None:
    ray.init(num_cpus=8, configure_logging=True, logging_level=ray.logging.WARNING)
    for DATASET in ["Diabetes", "Transfusion", "Parkinsons", "SPECT"]:
        objective = objective_function(AdaBoost)
        train = train_function(objective, DATASET)
        config = {
            "n_estimators": tune.choice([10, 50, 100, 200]),
            "learning_rate": tune.qloguniform(1e-5, 1, 5e-6),
        }
        cols = ["acc", *list(map(lambda k: f"config/{k}", config.keys()))]
        analysis = tune.run(train, config=config, num_samples=250)
        with capsys.disabled():
            print_results(analysis, cols, DATASET)


def test_lr_params(capsys: Any) -> None:
    ray.init(num_cpus=8, configure_logging=True, logging_level=ray.logging.WARNING)
    for DATASET in ["Diabetes", "Transfusion", "Parkinsons", "SPECT"]:

        objective = objective_function(LR, model_args=dict(solver="liblinear"))
        train = train_function(objective, DATASET)
        config = {
            "penalty": tune.choice(["l1", "l2"]),
            # "C": tune.qloguniform(1e-2, 10000, 5e-6),
            "C": tune.qloguniform(0.1, 2, 0.1),
            "max_iter": tune.choice([250, 500]),
        }
        cols = ["acc", *list(map(lambda k: f"config/{k}", config.keys()))]

        analysis = tune.run(train, config=config, num_samples=250)
        with capsys.disabled():
            print_results(analysis, cols, DATASET)


def test_svm_params(capsys: Any) -> None:
    ray.init(num_cpus=8, configure_logging=True, logging_level=ray.logging.WARNING)
    for DATASET in ["Diabetes", "Transfusion", "Parkinsons", "SPECT"]:

        objective = objective_function(SVC, model_args=dict(max_iter=500))
        train = train_function(objective, DATASET)
        config = {
            # "kernel": tune.choice(["linear", "poly", "rbf"]),
            "C": tune.qloguniform(10, 100, 0.5),
            # "C": tune.qloguniform(1, 5, 0.5),
            # "shrinking": tune.choice([True, False]),
        }
        cols = ["acc", *list(map(lambda k: f"config/{k}", config.keys()))]

        analysis = tune.run(train, config=config, num_samples=250)
        with capsys.disabled():
            df = print_results(analysis, cols, DATASET)
            ifg, ax = plt.subplots()
            sbn.scatterplot(data=df, x="C", y="acc", ax=ax)
            ax.set_xlabel("C")
            ax.set_ylabel("Accuracy")
            plt.show(block=False)
    plt.show()

